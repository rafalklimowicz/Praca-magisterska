{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ylcDInlZC8zh",
        "CuUS4FYWMjPO",
        "hIGKaON8sKUg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inicjalizacja"
      ],
      "metadata": {
        "id": "0XfaIegLC5hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicjalizacja zmiennych"
      ],
      "metadata": {
        "id": "m_ft6RNkC8k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "account = 1\n",
        "\n",
        "pickle_path_features_train_mal_8_mal = 'drive/My Drive/Data/features_train_mal_8_mal.pickle'\n",
        "pickle_path_features_train_mal_16_mal = 'drive/My Drive/Data/features_train_mal_16_mal.pickle'\n",
        "pickle_path_features_train_mal_32_mal = 'drive/My Drive/Data/features_train_mal_32_mal.pickle'\n",
        "pickle_path_features_test_mal_8_mal = 'drive/My Drive/Data/features_test_mal_8_mal.pickle'\n",
        "pickle_path_features_test_mal_16_mal = 'drive/My Drive/Data/features_test_mal_16_mal.pickle'\n",
        "pickle_path_features_test_mal_32_mal = 'drive/My Drive/Data/features_test_mal_32_mal.pickle'\n",
        "\n",
        "pickle_path_features_train_mal_8_ben = 'drive/My Drive/Data/features_train_mal_8_ben.pickle'\n",
        "pickle_path_features_train_mal_16_ben = 'drive/My Drive/Data/features_train_mal_16_ben.pickle'\n",
        "pickle_path_features_train_mal_32_ben = 'drive/My Drive/Data/features_train_mal_32_ben.pickle'\n",
        "pickle_path_features_test_mal_8_ben = 'drive/My Drive/Data/features_test_mal_8_ben.pickle'\n",
        "pickle_path_features_test_mal_16_ben = 'drive/My Drive/Data/features_test_mal_16_ben.pickle'\n",
        "pickle_path_features_test_mal_32_ben = 'drive/My Drive/Data/features_test_mal_32_ben.pickle'\n",
        "\n",
        "pickle_path_features_train_ben_8_mal = 'drive/My Drive/Data/features_train_ben_8_mal.pickle'\n",
        "pickle_path_features_train_ben_16_mal = 'drive/My Drive/Data/features_train_ben_16_mal.pickle'\n",
        "pickle_path_features_train_ben_32_mal = 'drive/My Drive/Data/features_train_ben_32_mal.pickle'\n",
        "pickle_path_features_test_ben_8_mal = 'drive/My Drive/Data/features_test_ben_8_mal.pickle'\n",
        "pickle_path_features_test_ben_16_mal = 'drive/My Drive/Data/features_test_ben_16_mal.pickle'\n",
        "pickle_path_features_test_ben_32_mal = 'drive/My Drive/Data/features_test_ben_32_mal.pickle'\n",
        "\n",
        "pickle_path_features_train_ben_8_ben = 'drive/My Drive/Data/features_train_ben_8_ben.pickle'\n",
        "pickle_path_features_train_ben_16_ben = 'drive/My Drive/Data/features_train_ben_16_ben.pickle'\n",
        "pickle_path_features_train_ben_32_ben = 'drive/My Drive/Data/features_train_ben_32_ben.pickle'\n",
        "pickle_path_features_test_ben_8_ben = 'drive/My Drive/Data/features_test_ben_8_ben.pickle'\n",
        "pickle_path_features_test_ben_16_ben = 'drive/My Drive/Data/features_test_ben_16_ben.pickle'\n",
        "pickle_path_features_test_ben_32_ben = 'drive/My Drive/Data/features_test_ben_32_ben.pickle'\n",
        "\n",
        "pickle_path_stats_train_mal_8_mal = 'drive/My Drive/Data/stats_train_mal_8_mal.pickle'\n",
        "pickle_path_stats_train_mal_16_mal = 'drive/My Drive/Data/stats_train_mal_16_mal.pickle'\n",
        "pickle_path_stats_train_mal_32_mal = 'drive/My Drive/Data/stats_train_mal_32_mal.pickle'\n",
        "pickle_path_stats_test_mal_8_mal = 'drive/My Drive/Data/stats_test_mal_8_mal.pickle'\n",
        "pickle_path_stats_test_mal_16_mal = 'drive/My Drive/Data/stats_test_mal_16_mal.pickle'\n",
        "pickle_path_stats_test_mal_32_mal = 'drive/My Drive/Data/stats_test_mal_32_mal.pickle'\n",
        "\n",
        "pickle_path_stats_train_mal_8_ben = 'drive/My Drive/Data/stats_train_mal_8_ben.pickle'\n",
        "pickle_path_stats_train_mal_16_ben = 'drive/My Drive/Data/stats_train_mal_16_ben.pickle'\n",
        "pickle_path_stats_train_mal_32_ben = 'drive/My Drive/Data/stats_train_mal_32_ben.pickle'\n",
        "pickle_path_stats_test_mal_8_ben = 'drive/My Drive/Data/stats_test_mal_8_ben.pickle'\n",
        "pickle_path_stats_test_mal_16_ben = 'drive/My Drive/Data/stats_test_mal_16_ben.pickle'\n",
        "pickle_path_stats_test_mal_32_ben = 'drive/My Drive/Data/stats_test_mal_32_ben.pickle'\n",
        "\n",
        "pickle_path_stats_train_ben_8_mal = 'drive/My Drive/Data/stats_train_ben_8_mal.pickle'\n",
        "pickle_path_stats_train_ben_16_mal = 'drive/My Drive/Data/stats_train_ben_16_mal.pickle'\n",
        "pickle_path_stats_train_ben_32_mal = 'drive/My Drive/Data/stats_train_ben_32_mal.pickle'\n",
        "pickle_path_stats_test_ben_8_mal = 'drive/My Drive/Data/stats_test_ben_8_mal.pickle'\n",
        "pickle_path_stats_test_ben_16_mal = 'drive/My Drive/Data/stats_test_ben_16_mal.pickle'\n",
        "pickle_path_stats_test_ben_32_mal = 'drive/My Drive/Data/stats_test_ben_32_mal.pickle'\n",
        "\n",
        "pickle_path_stats_train_ben_8_ben = 'drive/My Drive/Data/stats_train_ben_8_ben.pickle'\n",
        "pickle_path_stats_train_ben_16_ben = 'drive/My Drive/Data/stats_train_ben_16_ben.pickle'\n",
        "pickle_path_stats_train_ben_32_ben = 'drive/My Drive/Data/stats_train_ben_32_ben.pickle'\n",
        "pickle_path_stats_test_ben_8_ben = 'drive/My Drive/Data/stats_test_ben_8_ben.pickle'\n",
        "pickle_path_stats_test_ben_16_ben = 'drive/My Drive/Data/stats_test_ben_16_ben.pickle'\n",
        "pickle_path_stats_test_ben_32_ben = 'drive/My Drive/Data/stats_test_ben_32_ben.pickle'\n",
        "\n",
        "csv_path_stats = 'drive/My Drive/Data/CSV/acc' + str(account) + '_stats.csv'\n",
        "csv_path_features = 'drive/My Drive/Data/CSV/acc' + str(account) + '_features.csv'\n",
        "csv_path_both = 'drive/My Drive/Data/CSV/acc' + str(account) + '_both.csv'"
      ],
      "metadata": {
        "id": "MoHN8kIkDIX9"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicjalizacja bibliotek"
      ],
      "metadata": {
        "id": "ylcDInlZC8zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMqOxjvADIsn",
        "outputId": "6cd37fc3-922d-45f3-fd02-e1ae3e6070e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Przygotowanie danych"
      ],
      "metadata": {
        "id": "CuUS4FYWMjPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ładowanie residuum"
      ],
      "metadata": {
        "id": "mSmNnaGtnPzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Malignant"
      ],
      "metadata": {
        "id": "vkcW5-ywnOm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(pickle_path_stats_train_mal_8_mal, 'rb') as handle:\n",
        "  stats_train_mal_8_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_mal_8_mal, 'rb') as handle:\n",
        "  stats_test_mal_8_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_train_mal_8_ben, 'rb') as handle:\n",
        "  stats_train_mal_8_ben = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_mal_8_ben, 'rb') as handle:\n",
        "  stats_test_mal_8_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(stats_train_mal_8_mal)\n",
        "stats_train_mal_8_mal = scaler.transform(stats_train_mal_8_mal)\n",
        "stats_test_mal_8_mal = scaler.transform(stats_test_mal_8_mal)\n",
        "stats_train_mal_8_ben = scaler.transform(stats_train_mal_8_ben)\n",
        "stats_test_mal_8_ben = scaler.transform(stats_test_mal_8_ben)\n",
        "  \n",
        "with open(pickle_path_stats_train_mal_16_mal, 'rb') as handle:\n",
        "  stats_train_mal_16_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_mal_16_mal, 'rb') as handle:\n",
        "  stats_test_mal_16_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_train_mal_16_ben, 'rb') as handle:\n",
        "  stats_train_mal_16_ben = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_mal_16_ben, 'rb') as handle:\n",
        "  stats_test_mal_16_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(stats_train_mal_16_mal)\n",
        "stats_train_mal_16_mal = scaler.transform(stats_train_mal_16_mal)\n",
        "stats_test_mal_16_mal = scaler.transform(stats_test_mal_16_mal)\n",
        "stats_train_mal_16_ben = scaler.transform(stats_train_mal_16_ben)\n",
        "stats_test_mal_16_ben = scaler.transform(stats_test_mal_16_ben)\n",
        "\n",
        "with open(pickle_path_stats_train_mal_32_mal, 'rb') as handle:\n",
        "  stats_train_mal_32_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_mal_32_mal, 'rb') as handle:\n",
        "  stats_test_mal_32_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_train_mal_32_ben, 'rb') as handle:\n",
        "  stats_train_mal_32_ben = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_mal_32_ben, 'rb') as handle:\n",
        "  stats_test_mal_32_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(stats_train_mal_32_mal)\n",
        "stats_train_mal_32_mal = scaler.transform(stats_train_mal_32_mal)\n",
        "stats_test_mal_32_mal = scaler.transform(stats_test_mal_32_mal)\n",
        "stats_train_mal_32_ben = scaler.transform(stats_train_mal_32_ben)\n",
        "stats_test_mal_32_ben = scaler.transform(stats_test_mal_32_ben)"
      ],
      "metadata": {
        "id": "X8v8Sed1nOUe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benign"
      ],
      "metadata": {
        "id": "wtECY8qqnPGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(pickle_path_stats_train_ben_8_mal, 'rb') as handle:\n",
        "  stats_train_ben_8_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_ben_8_mal, 'rb') as handle:\n",
        "  stats_test_ben_8_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_train_ben_8_ben, 'rb') as handle:\n",
        "  stats_train_ben_8_ben = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_ben_8_ben, 'rb') as handle:\n",
        "  stats_test_ben_8_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(stats_train_ben_8_mal)\n",
        "stats_train_ben_8_mal = scaler.transform(stats_train_ben_8_mal)\n",
        "stats_test_ben_8_mal = scaler.transform(stats_test_ben_8_mal)\n",
        "stats_train_ben_8_ben = scaler.transform(stats_train_ben_8_ben)\n",
        "stats_test_ben_8_ben = scaler.transform(stats_test_ben_8_ben)\n",
        "  \n",
        "with open(pickle_path_stats_train_ben_16_mal, 'rb') as handle:\n",
        "  stats_train_ben_16_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_ben_16_mal, 'rb') as handle:\n",
        "  stats_test_ben_16_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_train_ben_16_ben, 'rb') as handle:\n",
        "  stats_train_ben_16_ben = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_ben_16_ben, 'rb') as handle:\n",
        "  stats_test_ben_16_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(stats_train_ben_16_mal)\n",
        "stats_train_ben_16_mal = scaler.transform(stats_train_ben_16_mal)\n",
        "stats_test_ben_16_mal = scaler.transform(stats_test_ben_16_mal)\n",
        "stats_train_ben_16_ben = scaler.transform(stats_train_ben_16_ben)\n",
        "stats_test_ben_16_ben = scaler.transform(stats_test_ben_16_ben)\n",
        "\n",
        "with open(pickle_path_stats_train_ben_32_mal, 'rb') as handle:\n",
        "  stats_train_ben_32_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_ben_32_mal, 'rb') as handle:\n",
        "  stats_test_ben_32_mal = pickle.load(handle)\n",
        "with open(pickle_path_stats_train_ben_32_ben, 'rb') as handle:\n",
        "  stats_train_ben_32_ben = pickle.load(handle)\n",
        "with open(pickle_path_stats_test_ben_32_ben, 'rb') as handle:\n",
        "  stats_test_ben_32_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(stats_train_ben_32_mal)\n",
        "stats_train_ben_32_mal = scaler.transform(stats_train_ben_32_mal)\n",
        "stats_test_ben_32_mal = scaler.transform(stats_test_ben_32_mal)\n",
        "stats_train_ben_32_ben = scaler.transform(stats_train_ben_32_ben)\n",
        "stats_test_ben_32_ben = scaler.transform(stats_test_ben_32_ben)"
      ],
      "metadata": {
        "id": "3zZUsTFfnWXI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ładowanie cech autoenkodera"
      ],
      "metadata": {
        "id": "QImzL84jhA_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Malignant"
      ],
      "metadata": {
        "id": "cevT27zFhA_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(pickle_path_features_train_mal_8_mal, 'rb') as handle:\n",
        "  features_train_mal_8_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_test_mal_8_mal, 'rb') as handle:\n",
        "  features_test_mal_8_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_train_mal_8_ben, 'rb') as handle:\n",
        "  features_train_mal_8_ben = pickle.load(handle)\n",
        "with open(pickle_path_features_test_mal_8_ben, 'rb') as handle:\n",
        "  features_test_mal_8_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(features_train_mal_8_mal)\n",
        "features_train_mal_8_mal = scaler.transform(features_train_mal_8_mal)\n",
        "features_test_mal_8_mal = scaler.transform(features_test_mal_8_mal)\n",
        "features_train_mal_8_ben = scaler.transform(features_train_mal_8_ben)\n",
        "features_test_mal_8_ben = scaler.transform(features_test_mal_8_ben)\n",
        "  \n",
        "with open(pickle_path_features_train_mal_16_mal, 'rb') as handle:\n",
        "  features_train_mal_16_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_test_mal_16_mal, 'rb') as handle:\n",
        "  features_test_mal_16_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_train_mal_16_ben, 'rb') as handle:\n",
        "  features_train_mal_16_ben = pickle.load(handle)\n",
        "with open(pickle_path_features_test_mal_16_ben, 'rb') as handle:\n",
        "  features_test_mal_16_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(features_train_mal_16_mal)\n",
        "features_train_mal_16_mal = scaler.transform(features_train_mal_16_mal)\n",
        "features_test_mal_16_mal = scaler.transform(features_test_mal_16_mal)\n",
        "features_train_mal_16_ben = scaler.transform(features_train_mal_16_ben)\n",
        "features_test_mal_16_ben = scaler.transform(features_test_mal_16_ben)\n",
        "\n",
        "with open(pickle_path_features_train_mal_32_mal, 'rb') as handle:\n",
        "  features_train_mal_32_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_test_mal_32_mal, 'rb') as handle:\n",
        "  features_test_mal_32_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_train_mal_32_ben, 'rb') as handle:\n",
        "  features_train_mal_32_ben = pickle.load(handle)\n",
        "with open(pickle_path_features_test_mal_32_ben, 'rb') as handle:\n",
        "  features_test_mal_32_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(features_train_mal_32_mal)\n",
        "features_train_mal_32_mal = scaler.transform(features_train_mal_32_mal)\n",
        "features_test_mal_32_mal = scaler.transform(features_test_mal_32_mal)\n",
        "features_train_mal_32_ben = scaler.transform(features_train_mal_32_ben)\n",
        "features_test_mal_32_ben = scaler.transform(features_test_mal_32_ben)"
      ],
      "metadata": {
        "id": "tlwDwJQihA_4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benign"
      ],
      "metadata": {
        "id": "kE9EtZDlhA_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(pickle_path_features_train_ben_8_mal, 'rb') as handle:\n",
        "  features_train_ben_8_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_test_ben_8_mal, 'rb') as handle:\n",
        "  features_test_ben_8_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_train_ben_8_ben, 'rb') as handle:\n",
        "  features_train_ben_8_ben = pickle.load(handle)\n",
        "with open(pickle_path_features_test_ben_8_ben, 'rb') as handle:\n",
        "  features_test_ben_8_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(features_train_ben_8_mal)\n",
        "features_train_ben_8_mal = scaler.transform(features_train_ben_8_mal)\n",
        "features_test_ben_8_mal = scaler.transform(features_test_ben_8_mal)\n",
        "features_train_ben_8_ben = scaler.transform(features_train_ben_8_ben)\n",
        "features_test_ben_8_ben = scaler.transform(features_test_ben_8_ben)\n",
        "  \n",
        "with open(pickle_path_features_train_ben_16_mal, 'rb') as handle:\n",
        "  features_train_ben_16_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_test_ben_16_mal, 'rb') as handle:\n",
        "  features_test_ben_16_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_train_ben_16_ben, 'rb') as handle:\n",
        "  features_train_ben_16_ben = pickle.load(handle)\n",
        "with open(pickle_path_features_test_ben_16_ben, 'rb') as handle:\n",
        "  features_test_ben_16_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(features_train_ben_16_mal)\n",
        "features_train_ben_16_mal = scaler.transform(features_train_ben_16_mal)\n",
        "features_test_ben_16_mal = scaler.transform(features_test_ben_16_mal)\n",
        "features_train_ben_16_ben = scaler.transform(features_train_ben_16_ben)\n",
        "features_test_ben_16_ben = scaler.transform(features_test_ben_16_ben)\n",
        "\n",
        "with open(pickle_path_features_train_ben_32_mal, 'rb') as handle:\n",
        "  features_train_ben_32_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_test_ben_32_mal, 'rb') as handle:\n",
        "  features_test_ben_32_mal = pickle.load(handle)\n",
        "with open(pickle_path_features_train_ben_32_ben, 'rb') as handle:\n",
        "  features_train_ben_32_ben = pickle.load(handle)\n",
        "with open(pickle_path_features_test_ben_32_ben, 'rb') as handle:\n",
        "  features_test_ben_32_ben = pickle.load(handle)\n",
        "scaler = preprocessing.StandardScaler().fit(features_train_ben_32_mal)\n",
        "features_train_ben_32_mal = scaler.transform(features_train_ben_32_mal)\n",
        "features_test_ben_32_mal = scaler.transform(features_test_ben_32_mal)\n",
        "features_train_ben_32_ben = scaler.transform(features_train_ben_32_ben)\n",
        "features_test_ben_32_ben = scaler.transform(features_test_ben_32_ben)"
      ],
      "metadata": {
        "id": "1yznPIIqhA_8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Połączenie statystyk i cech"
      ],
      "metadata": {
        "id": "2G7WnNg3ofVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "both_train_mal_8_mal = np.append(features_train_mal_8_mal, stats_train_mal_8_mal, axis=1)\n",
        "both_train_mal_8_ben = np.append(features_train_mal_8_ben, stats_train_mal_8_ben, axis=1)\n",
        "both_test_mal_8_mal = np.append(features_test_mal_8_mal, stats_test_mal_8_mal, axis=1)\n",
        "both_test_mal_8_ben = np.append(features_test_mal_8_ben, stats_test_mal_8_ben, axis=1)\n",
        "scaler = preprocessing.StandardScaler().fit(both_train_mal_8_mal)\n",
        "both_train_mal_8_mal = scaler.transform(both_train_mal_8_mal)\n",
        "both_test_mal_8_mal = scaler.transform(both_test_mal_8_mal)\n",
        "both_train_mal_8_ben = scaler.transform(both_train_mal_8_ben)\n",
        "both_test_mal_8_ben = scaler.transform(both_test_mal_8_ben)\n",
        "\n",
        "both_train_mal_16_mal = np.append(features_train_mal_16_mal, stats_train_mal_16_mal, axis=1)\n",
        "both_train_mal_16_ben = np.append(features_train_mal_16_ben, stats_train_mal_16_ben, axis=1)\n",
        "both_test_mal_16_mal = np.append(features_test_mal_16_mal, stats_test_mal_16_mal, axis=1) \n",
        "both_test_mal_16_ben = np.append(features_test_mal_16_ben, stats_test_mal_16_ben, axis=1) \n",
        "scaler = preprocessing.StandardScaler().fit(both_train_mal_16_mal)\n",
        "both_train_mal_16_mal = scaler.transform(both_train_mal_16_mal)\n",
        "both_test_mal_16_mal = scaler.transform(both_test_mal_16_mal)\n",
        "both_train_mal_16_ben = scaler.transform(both_train_mal_16_ben)\n",
        "both_test_mal_16_ben = scaler.transform(both_test_mal_16_ben)\n",
        "\n",
        "both_train_mal_32_mal = np.append(features_train_mal_32_mal, stats_train_mal_32_mal, axis=1)\n",
        "both_train_mal_32_ben = np.append(features_train_mal_32_ben, stats_train_mal_32_ben, axis=1)\n",
        "both_test_mal_32_mal = np.append(features_test_mal_32_mal, stats_test_mal_32_mal, axis=1)\n",
        "both_test_mal_32_ben = np.append(features_test_mal_32_ben, stats_test_mal_32_ben, axis=1)\n",
        "scaler = preprocessing.StandardScaler().fit(both_train_mal_32_mal)\n",
        "both_train_mal_32_mal = scaler.transform(both_train_mal_32_mal)\n",
        "both_test_mal_32_mal = scaler.transform(both_test_mal_32_mal)\n",
        "both_train_mal_32_ben = scaler.transform(both_train_mal_32_ben)\n",
        "both_test_mal_32_ben = scaler.transform(both_test_mal_32_ben)\n",
        "\n",
        "both_train_ben_8_mal = np.append(features_train_ben_8_mal, stats_train_ben_8_mal, axis=1)\n",
        "both_train_ben_8_ben = np.append(features_train_ben_8_ben, stats_train_ben_8_ben, axis=1)\n",
        "both_test_ben_8_mal = np.append(features_test_ben_8_mal, stats_test_ben_8_mal, axis=1)\n",
        "both_test_ben_8_ben = np.append(features_test_ben_8_ben, stats_test_ben_8_ben, axis=1)\n",
        "scaler = preprocessing.StandardScaler().fit(both_train_ben_8_mal)\n",
        "both_train_ben_8_mal = scaler.transform(both_train_ben_8_mal)\n",
        "both_test_ben_8_mal = scaler.transform(both_test_ben_8_mal)\n",
        "both_train_ben_8_ben = scaler.transform(both_train_ben_8_ben)\n",
        "both_test_ben_8_ben = scaler.transform(both_test_ben_8_ben)\n",
        "\n",
        "both_train_ben_16_mal = np.append(features_train_ben_16_mal, stats_train_ben_16_mal, axis=1)\n",
        "both_train_ben_16_ben = np.append(features_train_ben_16_ben, stats_train_ben_16_ben, axis=1)\n",
        "both_test_ben_16_mal = np.append(features_test_ben_16_mal, stats_test_ben_16_mal, axis=1) \n",
        "both_test_ben_16_ben = np.append(features_test_ben_16_ben, stats_test_ben_16_ben, axis=1) \n",
        "scaler = preprocessing.StandardScaler().fit(both_train_ben_16_mal)\n",
        "both_train_ben_16_mal = scaler.transform(both_train_ben_16_mal)\n",
        "both_test_ben_16_mal = scaler.transform(both_test_ben_16_mal)\n",
        "both_train_ben_16_ben = scaler.transform(both_train_ben_16_ben)\n",
        "both_test_ben_16_ben = scaler.transform(both_test_ben_16_ben)\n",
        "\n",
        "both_train_ben_32_mal = np.append(features_train_ben_32_mal, stats_train_ben_32_mal, axis=1)\n",
        "both_train_ben_32_ben = np.append(features_train_ben_32_ben, stats_train_ben_32_ben, axis=1)\n",
        "both_test_ben_32_mal = np.append(features_test_ben_32_mal, stats_test_ben_32_mal, axis=1)\n",
        "both_test_ben_32_ben = np.append(features_test_ben_32_ben, stats_test_ben_32_ben, axis=1)\n",
        "scaler = preprocessing.StandardScaler().fit(both_train_ben_32_mal)\n",
        "both_train_ben_32_mal = scaler.transform(both_train_ben_32_mal)\n",
        "both_test_ben_32_mal = scaler.transform(both_test_ben_32_mal)\n",
        "both_train_ben_32_ben = scaler.transform(both_train_ben_32_ben)\n",
        "both_test_ben_32_ben = scaler.transform(both_test_ben_32_ben)"
      ],
      "metadata": {
        "id": "RSEh_xQ3oc-p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Połączenie danych"
      ],
      "metadata": {
        "id": "56sCD9ZYnPCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats_train_mal_8 = np.vstack((stats_train_mal_8_ben, stats_train_mal_8_mal))\n",
        "stats_train_mal_16 = np.vstack((stats_train_mal_16_ben, stats_train_mal_16_mal))\n",
        "stats_train_mal_32 = np.vstack((stats_train_mal_32_ben, stats_train_mal_32_mal))\n",
        "\n",
        "stats_train_ben_8 = np.vstack((stats_train_ben_8_ben, stats_train_ben_8_mal))\n",
        "stats_train_ben_16 = np.vstack((stats_train_ben_16_ben, stats_train_ben_16_mal))\n",
        "stats_train_ben_32 = np.vstack((stats_train_ben_32_ben, stats_train_ben_32_mal))\n",
        "\n",
        "features_train_mal_8 = np.vstack((features_train_mal_8_ben, features_train_mal_8_mal))\n",
        "features_train_mal_16 = np.vstack((features_train_mal_16_ben, features_train_mal_16_mal))\n",
        "features_train_mal_32 = np.vstack((features_train_mal_32_ben, features_train_mal_32_mal))\n",
        "\n",
        "features_train_ben_8 = np.vstack((features_train_ben_8_ben, features_train_ben_8_mal))\n",
        "features_train_ben_16 = np.vstack((features_train_ben_16_ben, features_train_ben_16_mal))\n",
        "features_train_ben_32 = np.vstack((features_train_ben_32_ben, features_train_ben_32_mal))\n",
        "\n",
        "both_train_mal_8 = np.vstack((both_train_mal_8_ben, both_train_mal_8_mal))\n",
        "both_train_mal_16 = np.vstack((both_train_mal_16_ben, both_train_mal_16_mal))\n",
        "both_train_mal_32 = np.vstack((both_train_mal_32_ben, both_train_mal_32_mal))\n",
        "\n",
        "both_train_ben_8 = np.vstack((both_train_ben_8_ben, both_train_ben_8_mal))\n",
        "both_train_ben_16 = np.vstack((both_train_ben_16_ben, both_train_ben_16_mal))\n",
        "both_train_ben_32 = np.vstack((both_train_ben_32_ben, both_train_ben_32_mal))\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "stats_test_mal_8 = np.vstack((stats_test_mal_8_ben, stats_test_mal_8_mal))\n",
        "stats_test_mal_16 = np.vstack((stats_test_mal_16_ben, stats_test_mal_16_mal))\n",
        "stats_test_mal_32 = np.vstack((stats_test_mal_32_ben, stats_test_mal_32_mal))\n",
        "\n",
        "stats_test_ben_8 = np.vstack((stats_test_ben_8_ben, stats_test_ben_8_mal))\n",
        "stats_test_ben_16 = np.vstack((stats_test_ben_16_ben, stats_test_ben_16_mal))\n",
        "stats_test_ben_32 = np.vstack((stats_test_ben_32_ben, stats_test_ben_32_mal))\n",
        "\n",
        "features_test_mal_8 = np.vstack((features_test_mal_8_ben, features_test_mal_8_mal))\n",
        "features_test_mal_16 = np.vstack((features_test_mal_16_ben, features_test_mal_16_mal))\n",
        "features_test_mal_32 = np.vstack((features_test_mal_32_ben, features_test_mal_32_mal))\n",
        "\n",
        "features_test_ben_8 = np.vstack((features_test_ben_8_ben, features_test_ben_8_mal))\n",
        "features_test_ben_16 = np.vstack((features_test_ben_16_ben, features_test_ben_16_mal))\n",
        "features_test_ben_32 = np.vstack((features_test_ben_32_ben, features_test_ben_32_mal))\n",
        "\n",
        "both_test_mal_8 = np.vstack((both_test_mal_8_ben, both_test_mal_8_mal))\n",
        "both_test_mal_16 = np.vstack((both_test_mal_16_ben, both_test_mal_16_mal))\n",
        "both_test_mal_32 = np.vstack((both_test_mal_32_ben, both_test_mal_32_mal))\n",
        "\n",
        "both_test_ben_8 = np.vstack((both_test_ben_8_ben, both_test_ben_8_mal))\n",
        "both_test_ben_16 = np.vstack((both_test_ben_16_ben, both_test_ben_16_mal))\n",
        "both_test_ben_32 = np.vstack((both_test_ben_32_ben, both_test_ben_32_mal))"
      ],
      "metadata": {
        "id": "iIGcpFrUPFpK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = np.squeeze(np.vstack((np.zeros((features_train_mal_8_ben.shape[0], 1)), np.ones((features_train_mal_8_mal.shape[0], 1)))))\n",
        "test_y = np.squeeze(np.vstack((np.zeros((features_test_mal_8_ben.shape[0], 1)), np.ones((features_test_mal_8_mal.shape[0], 1)))))"
      ],
      "metadata": {
        "id": "p9O_ejFyhKtu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predykcja"
      ],
      "metadata": {
        "id": "9_QDDIQLqLwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = [\"Account\", \"Vector size\", \"Trained on\", \"Dataset\", \"Model\", \n",
        "                \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC\", \"AUC\"]\n",
        "df_stats = pd.DataFrame(columns = column_names)\n",
        "df_features = pd.DataFrame(columns = column_names)\n",
        "df_both = pd.DataFrame(columns = column_names)"
      ],
      "metadata": {
        "id": "ZXmXnGtlnvkO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statystyki"
      ],
      "metadata": {
        "id": "B84vgRKUK1ZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benign"
      ],
      "metadata": {
        "id": "m2Aj12n_eooz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_on = 'Benign'"
      ],
      "metadata": {
        "id": "4KV1EJ86e4gU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 8"
      ],
      "metadata": {
        "id": "34y1klbJeurx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 8\n",
        "x_train = stats_train_ben_8\n",
        "y_train = train_y\n",
        "x_test = stats_test_ben_8\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "jdzfmzlKLlbd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "jCcEPxYMfQEk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "tsYOxurBqJXv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "87rUnxNdqlwS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 16"
      ],
      "metadata": {
        "id": "VAVqTtGTr7CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 16\n",
        "x_train = stats_train_ben_16\n",
        "y_train = train_y\n",
        "x_test = stats_test_ben_16\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "2hHpnxqur7CH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "GC5BuycTr7CI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "D21ETfLKr7CK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "ED8vMdJZr7CL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 32"
      ],
      "metadata": {
        "id": "VozcCSRyr7aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 32\n",
        "x_train = stats_train_ben_32\n",
        "y_train = train_y\n",
        "x_test = stats_test_ben_32\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "DUkXBhXPr7aV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "--rymTGFr7aX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "Zr08Q8N1r7aZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "bQrAs9r0r7aa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Malignant"
      ],
      "metadata": {
        "id": "hIGKaON8sKUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_on = 'Malignant'"
      ],
      "metadata": {
        "id": "_3tNa9ehsKUh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 8"
      ],
      "metadata": {
        "id": "O-SxoA4dsKUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 8\n",
        "x_train = stats_train_mal_8\n",
        "y_train = train_y\n",
        "x_test = stats_test_mal_8\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "MhhTqWV3sKUj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "H8DCT20xsKUk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "5uAAeexMsKUl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "yzO_E891sKUm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 16"
      ],
      "metadata": {
        "id": "oXf-8yXrsKUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 16\n",
        "x_train = stats_train_mal_16\n",
        "y_train = train_y\n",
        "x_test = stats_test_mal_16\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "HQQOQcdGsKUo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "rNcxqoBrsKUq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "ztKajMH5sKUq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "bCTNQ3VssKUr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 32"
      ],
      "metadata": {
        "id": "wVzAkEFvsKUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 32\n",
        "x_train = stats_train_mal_32\n",
        "y_train = train_y\n",
        "x_test = stats_test_mal_32\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "g8hssJB1sKUr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "gzdMhiyHsKUs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "SdS_ANIzsKUs"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_stats.columns)\n",
        "df_stats = df_stats.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "cOfGm2kUsKUs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cechy"
      ],
      "metadata": {
        "id": "8cLvaZfwuW0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benign"
      ],
      "metadata": {
        "id": "GTHiCZ1euW0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_on = 'Benign'"
      ],
      "metadata": {
        "id": "izOYhBaduW0L"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 8"
      ],
      "metadata": {
        "id": "etZnPwaFuW0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 8\n",
        "x_train = features_train_ben_8\n",
        "y_train = train_y\n",
        "x_test = features_test_ben_8\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "h6e-YKWfuW0O"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "jouIKT1EuW0P"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "wamgQpE0uW0R"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "zDPGuoLauW0S"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 16"
      ],
      "metadata": {
        "id": "lewysn-RuW0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 16\n",
        "x_train = features_train_ben_16\n",
        "y_train = train_y\n",
        "x_test = features_test_ben_16\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "rVS_ooN1uW0U"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "FtF1CV5EuW0V"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "b3zF3kujuW0V"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "sjNyDkPOuW0X"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 32"
      ],
      "metadata": {
        "id": "ZM582egcuW0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 32\n",
        "x_train = features_train_ben_32\n",
        "y_train = train_y\n",
        "x_test = features_test_ben_32\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "aBy2E5ljuW0X"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "fyBh0IeCuW0Y"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "sifbRk0luW0Y"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "CZ3_Xm0HuW0Z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Malignant"
      ],
      "metadata": {
        "id": "-ecqGFmhuW0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_on = 'Malignant'"
      ],
      "metadata": {
        "id": "cbloZhw3uW0a"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 8"
      ],
      "metadata": {
        "id": "0olyW4OZuW0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 8\n",
        "x_train = features_train_mal_8\n",
        "y_train = train_y\n",
        "x_test = features_test_mal_8\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "WWnbxbf_uW0a"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "aqVlxabsuW0b"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "nHC-ogTJuW0b"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "60oL7dqZuW0c"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 16"
      ],
      "metadata": {
        "id": "qTu1UAsOuW0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 16\n",
        "x_train = features_train_mal_16\n",
        "y_train = train_y\n",
        "x_test = features_test_mal_16\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "86JjWMdvuW0d"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "yVMFrOKiuW0e"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "9WXMQDU6uW0e"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "w46fGte9uW0f"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 32"
      ],
      "metadata": {
        "id": "AQxDGTz7uW0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 32\n",
        "x_train = features_train_mal_32\n",
        "y_train = train_y\n",
        "x_test = features_test_mal_32\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "zksKpxueuW0g"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "0XyRwa1quW0h"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "7l3dyY4zuW0h"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_features.columns)\n",
        "df_features = df_features.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_features.columns)\n",
        "df_features = df_features.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "RyFD5rdDuW0i"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oba"
      ],
      "metadata": {
        "id": "OtjXyEG5uboq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benign"
      ],
      "metadata": {
        "id": "-pVNFYXBubor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_on = 'Benign'"
      ],
      "metadata": {
        "id": "9archFzMubos"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 8"
      ],
      "metadata": {
        "id": "-YB3En5Gubot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 8\n",
        "x_train = both_train_ben_8\n",
        "y_train = train_y\n",
        "x_test = both_test_ben_8\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "cUj66PuLubou"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "NEmu8lhyubov"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "dTDkgBftuboz"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "GNEQvqKaubo1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 16"
      ],
      "metadata": {
        "id": "Il2rHPJLubo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 16\n",
        "x_train = both_train_ben_16\n",
        "y_train = train_y\n",
        "x_test = both_test_ben_16\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "4ORILWIbubo4"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "uRIpAbkyubo5"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "9i08Ql3Nubo7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "ejsYhMQiubo8"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 32"
      ],
      "metadata": {
        "id": "cKBTWp5eubo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 32\n",
        "x_train = both_train_ben_32\n",
        "y_train = train_y\n",
        "x_test = both_test_ben_32\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "IRPHGunCubo9"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "1Grxt3dIubo-"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "ZCSCztNhubo-"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "cnGWPgZZubo-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Malignant"
      ],
      "metadata": {
        "id": "TSwluXsTubo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_on = 'Malignant'"
      ],
      "metadata": {
        "id": "8nj3uHuKubo_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 8"
      ],
      "metadata": {
        "id": "9nzUJhoxubpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 8\n",
        "x_train = both_train_mal_8\n",
        "y_train = train_y\n",
        "x_test = both_test_mal_8\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "A3WZHANUubpA"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "OQXp3BHpubpC"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "dLeCnAFrubpD"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "z_9c5UW3ubpD"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 16"
      ],
      "metadata": {
        "id": "L68-uPK9ubpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 16\n",
        "x_train = both_train_mal_16\n",
        "y_train = train_y\n",
        "x_test = both_test_mal_16\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "9Ly9TF0HubpF"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "RvCb8HoSubpG"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "KQ9bHrT1ubpG"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "O-zsmrVoubpH"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 32"
      ],
      "metadata": {
        "id": "DoIN8hRoubpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 32\n",
        "x_train = both_train_mal_32\n",
        "y_train = train_y\n",
        "x_test = both_test_mal_32\n",
        "y_test = test_y"
      ],
      "metadata": {
        "id": "g2GDvmA1ubpH"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression'\n",
        "model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "4QA-F1vJubpI"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'SVM'\n",
        "model = SVC(probability = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.predict_proba(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba[:,1])\n",
        "train_auc = roc_auc_score(y_train, train_proba[:,1])\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.predict_proba(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba[:,1])\n",
        "test_auc = roc_auc_score(y_test, test_proba[:,1])\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "I9CPuGflubpI"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Perceptron'\n",
        "model = Perceptron(tol=1e-3, random_state=0, shuffle = True)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "dataset = 'Train'\n",
        "\n",
        "train_pred = model.predict(x_train)\n",
        "train_proba = model.decision_function(x_train)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "train_precision = precision_score(y_train, train_pred)\n",
        "train_recall = recall_score(y_train, train_pred)\n",
        "train_f1score = f1_score(y_train, train_pred)\n",
        "train_roc = roc_curve(y_train, train_proba)\n",
        "train_auc = roc_auc_score(y_train, train_proba)\n",
        "\n",
        "train_append = [account, vector_size, trained_on, dataset, model_name, train_accuracy, \n",
        "                train_precision, train_recall, train_f1score, [a.tolist() for a in list(train_roc)], train_auc]\n",
        "train_series = pd.Series(train_append, index = df_both.columns)\n",
        "df_both = df_both.append(train_series, ignore_index=True)\n",
        "\n",
        "dataset = 'Test'\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "test_proba = model.decision_function(x_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "test_precision = precision_score(y_test, test_pred)\n",
        "test_recall = recall_score(y_test, test_pred)\n",
        "test_f1score = f1_score(y_test, test_pred)\n",
        "test_roc = roc_curve(y_test, test_proba)\n",
        "test_auc = roc_auc_score(y_test, test_proba)\n",
        "\n",
        "test_append = [account, vector_size, trained_on, dataset, model_name, test_accuracy, \n",
        "                test_precision, test_recall, test_f1score, [a.tolist() for a in list(test_roc)], test_auc]\n",
        "test_series = pd.Series(test_append, index = df_both.columns)\n",
        "df_both = df_both.append(test_series, ignore_index=True)"
      ],
      "metadata": {
        "id": "GhsNHkjkubpJ"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zapisanie do CSV"
      ],
      "metadata": {
        "id": "Ig7BleS9yN33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_stats.to_csv(csv_path_stats, index=False)\n",
        "df_features.to_csv(csv_path_features, index=False)\n",
        "df_both.to_csv(csv_path_both, index=False)"
      ],
      "metadata": {
        "id": "9HZkOTfVyL_Z"
      },
      "execution_count": 144,
      "outputs": []
    }
  ]
}